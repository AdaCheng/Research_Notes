# Language Model

## Dataset

## Recommendation
### A Neural Probabilistic Language Model (TODO)
 Yoshua Bengio, Rejean Ducharme, Pascal Vincent, Christian Jauvin
 *Departement d'Informatique et Recherche Operationnelle*
 *JMLR'03*

- [Link of Paper (Citation: 4,750)](http://www.jmlr.org/papers/v3/bengio03a)

### Efficient Estimation of Word Representations in Vector Space (TODO)
 Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean  
 *Google*  
 *Computer Science'13*

 - [Link of Paper (Citation: 10,678)](https://arxiv.org/abs/1301.3781)
 - Link of Note

### Distributed Representations of Words and Phrases and their Compositionality (TODO)  
 Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean  
 *Google*  
 *NIPS'13*
 
 - [Link of Paper (Citation: 13,124)](http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases)
 - [Link of Note]

### GloVe: Global Vectors for Word Representation (TODO)
 Jeffrey Pennington, Richard Socher, Christopher D. Mannning  
 *Computer Science Department, Stanford University*  
 *EMNLP'14*

 - [Link of Paper (Citation: 7,915)](https://www.aclweb.org/anthology/D14-1162)
 - Link of Note

### Deep contextualized word representations (TODO)
 Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer  
 *Allen Institue for artificial Intelligence*  
 *ACL'18*

 - [Link of Paper (Citation: 788)](https://arxiv.org/abs/1802.05365)
 - Link of Note

### Improving Language Understanding by Generative Pre-Training (TODO)
 Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever  
 *OpenAI*  

 - [Link of Paper (Citation: 177)](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
 - Link of Note

### BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (TODO)
 Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova  
 *Google AI Language*  
 *NAACL'19*

 - [Link of Paper (Citation: 1,032)](https://arxiv.org/abs/1810.04805)
 - Link of Note

## Paper
